# Prometheus Alerting Rules Template
# Place this file in your Prometheus rules directory and reference it in prometheus.yml:
#   rule_files:
#     - "alert-rules.yml"

groups:
  - name: service_availability
    rules:
      - alert: ServiceDown
        expr: up{job=~".+"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} on {{ $labels.instance }} has been unreachable for more than 1 minute."
          runbook_url: "https://wiki.example.com/runbooks/service-down"

      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has a 5xx error rate above 5% for the last 5 minutes (current: {{ $value | humanizePercentage }})."
          runbook_url: "https://wiki.example.com/runbooks/high-error-rate"

      - alert: HighLatencyP99
        expr: |
          histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[10m])) by (le, job))
          > 2.0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High p99 latency on {{ $labels.job }}"
          description: "{{ $labels.job }} p99 latency is above 2 seconds for the last 10 minutes (current: {{ $value | humanizeDuration }})."
          runbook_url: "https://wiki.example.com/runbooks/high-latency"

      - alert: HighLatencyP95
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))
          > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Elevated p95 latency on {{ $labels.job }}"
          description: "{{ $labels.job }} p95 latency exceeds 1 second for the last 5 minutes."

  - name: resource_usage
    rules:
      - alert: HighCpuUsage
        expr: |
          (
            sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
            /
            sum(container_spec_cpu_quota / container_spec_cpu_period) by (pod)
          ) > 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.pod }}"
          description: "Pod {{ $labels.pod }} CPU usage is above 85% of its limit for the last 10 minutes."

      - alert: HighMemoryUsage
        expr: |
          (
            sum(container_memory_working_set_bytes) by (pod)
            /
            sum(container_spec_memory_limit_bytes) by (pod)
          ) > 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.pod }}"
          description: "Pod {{ $labels.pod }} memory usage is above 85% of its limit for the last 10 minutes."

      - alert: HighMemoryUsageCritical
        expr: |
          (
            sum(container_memory_working_set_bytes) by (pod)
            /
            sum(container_spec_memory_limit_bytes) by (pod)
          ) > 0.95
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical memory usage on {{ $labels.pod }}"
          description: "Pod {{ $labels.pod }} memory usage is above 95% of its limit. OOM kill is imminent."
          runbook_url: "https://wiki.example.com/runbooks/memory-leak"

  - name: business_metrics
    rules:
      - alert: OrderProcessingFailureRate
        expr: |
          (
            sum(rate(business_orders_total{status="failed"}[10m]))
            /
            sum(rate(business_orders_total[10m]))
          ) > 0.10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High order failure rate"
          description: "More than 10% of orders are failing for the last 5 minutes."
